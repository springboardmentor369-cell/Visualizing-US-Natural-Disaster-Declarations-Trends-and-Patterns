# Week 4: Importing Live Data into Power BI
This week focused on understanding live data in Power BI and how real-time or continuously updating data can be imported and managed. This week was very important because it helped me understand how data works in real business environments, where information changes every minute and dashboards need to update automatically. 

Along with live data concepts, we were also introduced to GitHub to showcase our proof of work professionally.
## Understanding Live Data and Its Importance
At the beginning of the week, we learned what importing live data actually means. Live data refers to data that keeps updating at regular intervals or in real time, instead of being static. This is important because in real-world scenarios, data such as sales, customer tickets, transactions, or disaster updates keeps increasing every hour or every second.

This made me understand that manual data updates are not practical, and automation becomes necessary to handle such continuously changing data.
## Introduction to API and Its Role in Power BI
We were then introduced to the concept of API (Application Programming Interface). I learned that an API acts as a bridge that allows different systems or applications to communicate and exchange data.

In the context of Power BI, APIs are important because they allow us to fetch live data directly from platforms or systems. This helped me understand how modern dashboards stay updated without manual intervention.
## Ways to Import Live Data into Power BI
We were asked to explore different ways of importing live data. The first method we learned was importing data from the Web using a URL.

While imorting data from **NSE website**, I learned an important limitation: not every website allows data access. 
Due to security, privacy, and ethical reasons, many websites restrict access unless we have proper authorization or an organization ID. This helped me understand why accessing real-time data is not always straightforward.
## Understanding Data Refresh and Version Limitations
Another important learning was understanding how Power BI versions affect data refresh options. I learned that:
* The free version does not allow automatic refresh
* The pro version allows up to 8 refreshes per day
This made it clear that tool limitations also play a role when working with live or frequently updating data.
## Live Data vs Disaster Dataset Context
Since we are working on **disaster-related datasets**, we understood that live data may not always be useful because disasters do not occur every second. However, live data is extremely useful in cases like:
* Customer support systems
* Banking transactions
* Sales and CRM platforms

To understand this concept better, sir introduced us to the **Zendesk** platform, which helped us visualize how live data works in real-time systems such as customer ticketing and support.
## Synthetic Data and Dummy Data
We also learned the difference between dummy data and synthetic data. Dummy data is simple sample data, while synthetic data is created in a way that closely resembles real-world data.

This concept was important because when real APIs are not accessible, we can create synthetic data or sample APIs to practice live data concepts without violating security or ethical rules.
## Understanding ETL Pipelines
Another key concept introduced this week was ETL pipelines, which stand for Extract, Transform, and Load. I learned that ETL pipelines automate the entire data workflow:
* Extracting data from sources
* Transforming it into a usable format
* Loading it into Power BI
This helped me understand how dashboards automatically update whenever the data changes, without repeating the entire process manually.
## Importance of Proof of Work and Introduction to GitHub
Towards the end of the week, we learned that doing a project is not enough; showing proof of work is equally important. For this purpose, we were introduced to GitHub as a platform to showcase our projects in a structured and professional way.
### GitHub Repository Structure Learned
We were guided to create a proper GitHub repository by following specific steps:
* Creating a project repository and keeping it public
* Naming the project clearly and professionally
* Creating five main folders inside the repository:
#### Data: 
Containing raw data and processed data
#### Power BI: 
Containing Power BI files and dashboards
#### Data Cleaning: 
Containing the complete cleaning journey screenshots and related files
#### Screenshots: 
Containing labelled screenshots to show project flow clearly
#### Documentation: 
Containing week-wise learning files and a README file
We also learned that the README file is extremely important, as it gives the first impression of the entire project and explains everything clearly to the viewer.
## Key Learnings from Week 4
* Live data helps in building automated and real-time dashboards
* APIs play a crucial role in fetching live data
* Not all data sources are accessible due to security reasons
* ETL pipelines automate data wo
